# HW3 - Proabilistic ContextFree Grammar

### Task 1
For example, for sentence `Arthur is the king .` we have `P(T, S)=1*1/6*(1/7)^5*(1/8)*(1/6)*(1/9)*(1/21)*(1/183)=5.973*10^-12`  
Sentence:
```
P(Arthur|Proper)P(is|VerbT)P(the|Det)P(king|Noun)P(.|Misc) 
= (1/8)*(1/6)*(1/9)*(1/21)*(1/183)
```
Parse:
```
P(S2|ROOT)P(+Proper|S2)P(Proper +VerbT|+Proper)P(VerbT +Det|+VerbT)P(Det +Noun|+Det)P(Noun +Misc|+Noun)P(Misc|+Misc)
= 1*1/6*(1/7)^5
```
In the parsing part, PCFG uses bigram probabilities to predict probability of each parsing tree.
Therefore, this PCFG implements a bigram language model. 

### Task 2
The main difference is that the PCFG that only contains grammar 1 cannot parse most of the sentences. This is because grammar 1 does not have rules such as S1 -> +Misc to parse sentences that begin with a *Misc* word. So if we use grammar 1 and 2 together, grammar 2 will work like supplementary grammar to grammar 1, but grammar 1 has priority.

### Task 3
When we only use grammar 2 to generate sentence, we will get very strange sentences. That is because in grammar 2 we have the same weight for each rule.  Grammar 1 has different weight for different parsing rules, which makes the generated sentence makes better sense in common grammar. In grammar 2, an arbitrary tag could be followed by any tags used in grammar 2 with same probabilities. A sentence generated by this grammar would be simply a random combination of words.  
However, since ROOT->S1 is 99 and ROOT->S2 is 1, most of sentences will be generated using grammar 1 though we use grammar 1 and grammar 2 together. It is almost the same as using merely grammar 1 to generate sentences.
